{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz1QJnAqcZdZ",
        "outputId": "fc9c7e3e-9e0a-4d7c-c158-a55949bb597a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4OshZVeSy9eq",
        "outputId": "bcf82dff-8a3a-4287-b69a-d9c175ad3c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.194-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.17-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.194-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.17-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.194 ultralytics-thop-2.0.17\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# avijit labelling style\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def generate_unique_colors(num_classes):\n",
        "    \"\"\"Generate visually distinct colors for each class\"\"\"\n",
        "    colors = []\n",
        "\n",
        "    # Pre-defined distinct colors for common classes\n",
        "    predefined_colors = [\n",
        "        (255, 0, 0),     # Red\n",
        "        (0, 255, 0),     # Green\n",
        "        (0, 0, 255),     # Blue\n",
        "        (255, 255, 0),   # Yellow\n",
        "        (255, 0, 255),   # Magenta\n",
        "        (0, 255, 255),   # Cyan\n",
        "        (255, 165, 0),   # Orange\n",
        "        (128, 0, 128),   # Purple\n",
        "        (255, 192, 203), # Pink\n",
        "        (0, 128, 0),     # Dark Green\n",
        "        (128, 128, 0),   # Olive\n",
        "        (0, 0, 128),     # Navy\n",
        "        (128, 0, 0),     # Maroon\n",
        "        (255, 20, 147),  # Deep Pink\n",
        "        (32, 178, 170),  # Light Sea Green\n",
        "        (255, 69, 0),    # Red Orange\n",
        "        (138, 43, 226),  # Blue Violet\n",
        "        (34, 139, 34),   # Forest Green\n",
        "        (220, 20, 60),   # Crimson\n",
        "        (255, 215, 0),   # Gold\n",
        "    ]\n",
        "\n",
        "    # Use predefined colors first\n",
        "    for i in range(min(num_classes, len(predefined_colors))):\n",
        "        colors.append(predefined_colors[i])\n",
        "\n",
        "    # Generate additional colors if needed using HSV color space\n",
        "    if num_classes > len(predefined_colors):\n",
        "        for i in range(len(predefined_colors), num_classes):\n",
        "            # Create colors in HSV space for better distribution\n",
        "            hue = int((i - len(predefined_colors)) * 360 / (num_classes - len(predefined_colors)))\n",
        "            saturation = 255\n",
        "            value = 255\n",
        "\n",
        "            # Convert HSV to RGB\n",
        "            hsv = np.uint8([[[hue, saturation, value]]])\n",
        "            rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)[0][0]\n",
        "            colors.append(tuple(map(int, rgb)))\n",
        "\n",
        "    return colors\n",
        "\n",
        "def regions_overlap(region1, region2):\n",
        "    \"\"\"Check if two rectangular regions overlap\"\"\"\n",
        "    x1, y1, w1, h1 = region1\n",
        "    x2, y2, w2, h2 = region2\n",
        "\n",
        "    return not (x1 + w1 <= x2 or x2 + w2 <= x1 or y1 + h1 <= y2 or y2 + h2 <= y1)\n",
        "\n",
        "def draw_text_with_background(img, text, position, font_scale=0.5, font_thickness=1,\n",
        "                             text_color=(255, 255, 255), bg_color=(0, 0, 0),\n",
        "                             bg_alpha=0.7):\n",
        "    \"\"\"Draw text with semi-transparent background\"\"\"\n",
        "    x, y = position\n",
        "\n",
        "    # Ensure coordinates are within image bounds\n",
        "    if x < 0 or y < 0 or x >= img.shape[1] or y >= img.shape[0]:\n",
        "        return\n",
        "\n",
        "    # Get text size\n",
        "    (text_width, text_height), baseline = cv2.getTextSize(\n",
        "        text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness\n",
        "    )\n",
        "\n",
        "    # Create background rectangle with bounds checking\n",
        "    padding = 2\n",
        "    bg_x1 = max(0, x - padding)\n",
        "    bg_y1 = max(0, y - text_height - padding)\n",
        "    bg_x2 = min(img.shape[1], x + text_width + padding)\n",
        "    bg_y2 = min(img.shape[0], y + baseline + padding)\n",
        "\n",
        "    # Only draw if rectangle is valid\n",
        "    if bg_x2 > bg_x1 and bg_y2 > bg_y1:\n",
        "        # Draw semi-transparent background\n",
        "        overlay = img.copy()\n",
        "        cv2.rectangle(overlay, (bg_x1, bg_y1), (bg_x2, bg_y2), bg_color, -1)\n",
        "        cv2.addWeighted(overlay, bg_alpha, img, 1 - bg_alpha, 0, img)\n",
        "\n",
        "        # Draw text\n",
        "        cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    font_scale, text_color, font_thickness)\n",
        "\n",
        "def find_non_overlapping_position(boxes, confidences, class_ids, class_names, img_shape):\n",
        "    \"\"\"Find non-overlapping positions for text labels\"\"\"\n",
        "    text_positions = []\n",
        "    occupied_regions = []\n",
        "\n",
        "    img_height, img_width = img_shape[:2]\n",
        "\n",
        "    for i, (box, conf, class_id) in enumerate(zip(boxes, confidences, class_ids)):\n",
        "        x, y, w, h = box\n",
        "\n",
        "        # Bounds checking for box coordinates\n",
        "        x = max(0, min(x, img_width - 1))\n",
        "        y = max(0, min(y, img_height - 1))\n",
        "        w = max(1, min(w, img_width - x))\n",
        "        h = max(1, min(h, img_height - y))\n",
        "\n",
        "        # Safe class name retrieval\n",
        "        if isinstance(class_names, dict):\n",
        "            class_name = class_names.get(class_id, f\"Class {class_id}\")\n",
        "        elif isinstance(class_names, list) and class_id < len(class_names):\n",
        "            class_name = class_names[class_id]\n",
        "        else:\n",
        "            class_name = f\"Class {class_id}\"\n",
        "\n",
        "        text = f\"{class_name}: {conf:.2f}\"\n",
        "\n",
        "        # Calculate text size\n",
        "        font_scale = 0.5\n",
        "        font_thickness = 1\n",
        "        (text_width, text_height), baseline = cv2.getTextSize(\n",
        "            text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness\n",
        "        )\n",
        "\n",
        "        # Try different positions in order of preference\n",
        "        candidate_positions = [\n",
        "            (x, y - 5),                           # Above box (default)\n",
        "            (x, y + h + text_height + 5),         # Below box\n",
        "            (x + w + 5, y + text_height),         # Right of box\n",
        "            (x - text_width - 5, y + text_height), # Left of box\n",
        "            (x + w//2 - text_width//2, y + h//2 + text_height//2), # Center of box\n",
        "        ]\n",
        "\n",
        "        text_pos = None\n",
        "        for pos_x, pos_y in candidate_positions:\n",
        "            # Check boundaries\n",
        "            if (pos_x < 0 or pos_y < 0 or\n",
        "                pos_x + text_width > img_width or\n",
        "                pos_y > img_height):\n",
        "                continue\n",
        "\n",
        "            # Create text region\n",
        "            text_region = [pos_x, pos_y - text_height, text_width, text_height]\n",
        "\n",
        "            # Check if this region overlaps with any existing text\n",
        "            overlap = False\n",
        "            for occupied in occupied_regions:\n",
        "                if regions_overlap(text_region, occupied):\n",
        "                    overlap = True\n",
        "                    break\n",
        "\n",
        "            if not overlap:\n",
        "                text_pos = (pos_x, pos_y)\n",
        "                occupied_regions.append(text_region)\n",
        "                break\n",
        "\n",
        "        # If no position found, use offset position\n",
        "        if text_pos is None:\n",
        "            offset_y = len(occupied_regions) * (text_height + 5)\n",
        "            text_pos = (x, max(text_height + 5, y - 5 - offset_y))\n",
        "            occupied_regions.append([x, text_pos[1] - text_height, text_width, text_height])\n",
        "\n",
        "        text_positions.append((text_pos, text, font_scale, font_thickness))\n",
        "\n",
        "    return text_positions\n",
        "\n",
        "def draw_hierarchical_labels(img, boxes, confidences, class_ids, class_names):\n",
        "    \"\"\"Draw labels in a hierarchical manner to avoid overlaps\"\"\"\n",
        "    if len(boxes) == 0:\n",
        "        return img\n",
        "\n",
        "    # Sort by confidence (highest first)\n",
        "    sorted_indices = np.argsort(confidences)[::-1]\n",
        "\n",
        "    # Create layers for text positioning\n",
        "    text_layers = []\n",
        "    layer_height = 25\n",
        "\n",
        "    for idx in sorted_indices:\n",
        "        box = boxes[idx]\n",
        "        conf = confidences[idx]\n",
        "        class_id = class_ids[idx]\n",
        "\n",
        "        x, y, w, h = box\n",
        "\n",
        "        # Safe class name retrieval\n",
        "        if isinstance(class_names, dict):\n",
        "            class_name = class_names.get(class_id, f\"Class {class_id}\")\n",
        "        elif isinstance(class_names, list) and class_id < len(class_names):\n",
        "            class_name = class_names[class_id]\n",
        "        else:\n",
        "            class_name = f\"Class {class_id}\"\n",
        "\n",
        "        text = f\"{class_name}: {conf:.2f}\"\n",
        "\n",
        "        # Find appropriate layer\n",
        "        layer = 0\n",
        "        text_y = y - 5 - (layer * layer_height)\n",
        "\n",
        "        # Check if this position conflicts with existing text in this layer\n",
        "        while any(abs(existing_x - x) < 100 and existing_layer == layer\n",
        "                 for existing_x, existing_layer in text_layers):\n",
        "            layer += 1\n",
        "            text_y = y - 5 - (layer * layer_height)\n",
        "\n",
        "        # Ensure text doesn't go above image\n",
        "        if text_y < 20:\n",
        "            text_y = y + h + 20 + (layer * layer_height)\n",
        "\n",
        "        text_layers.append((x, layer))\n",
        "\n",
        "        # Draw text with background\n",
        "        draw_text_with_background(img, text, (x, text_y))\n",
        "\n",
        "    return img\n",
        "\n",
        "def draw_detections_with_side_panel(img, boxes, confidences, class_ids, class_names, colors):\n",
        "    \"\"\"Draw detections with labels in a side panel (only for safety violations)\"\"\"\n",
        "    panel_width = 250\n",
        "    img_height, img_width = img.shape[:2]\n",
        "\n",
        "    # Create extended image with side panel\n",
        "    extended_img = np.zeros((img_height, img_width + panel_width, 3), dtype=np.uint8)\n",
        "    extended_img[:, :img_width] = img\n",
        "    extended_img[:, img_width:] = (50, 50, 50)  # Dark gray panel\n",
        "\n",
        "    # Define safety violation classes\n",
        "    safety_violations = [\n",
        "        #'helmet', 'Safety_Vest', 'Safety_Vest', 'Safety_goggles','Safety_shoes'\n",
        "        'no_helmet', 'no_vest', #'no_goggles', #'No_SafetyShoes',\n",
        "        'Person', #'Pvc_Suit', 'No_Pvc_suit', 'witty shoeshout safe'\n",
        "    ]\n",
        "\n",
        "    panel_item_count = 0\n",
        "\n",
        "    for i, (box, conf, class_id) in enumerate(zip(boxes, confidences, class_ids)):\n",
        "        x, y, w, h = box\n",
        "\n",
        "        # Bounds checking\n",
        "        x = max(0, min(x, img_width - 1))\n",
        "        y = max(0, min(y, img_height - 1))\n",
        "        w = max(1, min(w, img_width - x))\n",
        "        h = max(1, min(h, img_height - y))\n",
        "\n",
        "        # Use consistent color for this class\n",
        "        color = colors[class_id] if class_id < len(colors) else colors[class_id % len(colors)]\n",
        "\n",
        "        # Draw bounding box (for all detections)\n",
        "        cv2.rectangle(extended_img, (x, y), (x + w, y + h), color, 2)\n",
        "\n",
        "        # Get class name\n",
        "        if isinstance(class_names, dict):\n",
        "            class_name = class_names.get(class_id, f\"Class {class_id}\")\n",
        "        elif isinstance(class_names, list) and class_id < len(class_names):\n",
        "            class_name = class_names[class_id]\n",
        "        else:\n",
        "            class_name = f\"Class {class_id}\"\n",
        "\n",
        "        # Check if this is a safety violation\n",
        "        is_safety_violation = any(violation.lower() in class_name.lower() for violation in safety_violations)\n",
        "\n",
        "        if is_safety_violation:\n",
        "            # Draw number on box for safety violations only\n",
        "            cv2.putText(extended_img, str(panel_item_count + 1), (x+5, y+20),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "\n",
        "            # Draw label in side panel\n",
        "            label_text = f\"{panel_item_count + 1}. {class_name}: {conf:.2f}\"\n",
        "\n",
        "            panel_y = 30 + panel_item_count * 25\n",
        "            if panel_y < img_height:  # Check bounds\n",
        "                cv2.putText(extended_img, label_text, (img_width + 10, panel_y),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "\n",
        "            panel_item_count += 1\n",
        "\n",
        "    return extended_img\n",
        "\n",
        "def draw_yolo_detections(img, boxes, confidences, class_ids, class_names, colors,\n",
        "                        method='smart_positioning'):\n",
        "    \"\"\"Draw YOLO detections with selective text labels for safety violations only\"\"\"\n",
        "    if len(boxes) == 0:\n",
        "        return img\n",
        "\n",
        "    # Define safety violation classes that should show text\n",
        "    safety_violations = [\n",
        "        #'helmet', 'Safety_Vest', 'Safety_Vest', 'Safety_goggles','Safety_shoes'\n",
        "        'no_helmet', 'no_vest', 'no_goggles', 'No_SafetyShoes',\n",
        "        'Person', #'Pvc_Suit', 'No_Pvc_suit', 'witty shoeshout safe'\n",
        "    ]\n",
        "\n",
        "    # Draw bounding boxes first (for ALL detections)\n",
        "    for i, (box, conf, class_id) in enumerate(zip(boxes, confidences, class_ids)):\n",
        "        x, y, w, h = box\n",
        "\n",
        "        # Bounds checking\n",
        "        img_height, img_width = img.shape[:2]\n",
        "        x = max(0, min(x, img_width - 1))\n",
        "        y = max(0, min(y, img_height - 1))\n",
        "        w = max(1, min(w, img_width - x))\n",
        "        h = max(1, min(h, img_height - y))\n",
        "\n",
        "        # Use consistent color for this class\n",
        "        color = colors[class_id] if class_id < len(colors) else colors[class_id % len(colors)]\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
        "\n",
        "    # Filter boxes and data for safety violations only\n",
        "    safety_boxes = []\n",
        "    safety_confidences = []\n",
        "    safety_class_ids = []\n",
        "\n",
        "    for i, (box, conf, class_id) in enumerate(zip(boxes, confidences, class_ids)):\n",
        "        # Get class name\n",
        "        if isinstance(class_names, dict):\n",
        "            class_name = class_names.get(class_id, f\"Class {class_id}\")\n",
        "        elif isinstance(class_names, list) and class_id < len(class_names):\n",
        "            class_name = class_names[class_id]\n",
        "        else:\n",
        "            class_name = f\"Class {class_id}\"\n",
        "\n",
        "        # Check if this is a safety violation\n",
        "        if any(violation.lower() in class_name.lower() for violation in safety_violations):\n",
        "            safety_boxes.append(box)\n",
        "            safety_confidences.append(conf)\n",
        "            safety_class_ids.append(class_id)\n",
        "\n",
        "    # Draw text labels only for safety violations\n",
        "    if len(safety_boxes) > 0:\n",
        "        if method == 'smart_positioning':\n",
        "            text_positions = find_non_overlapping_position(\n",
        "                safety_boxes, safety_confidences, safety_class_ids, class_names, img.shape\n",
        "            )\n",
        "\n",
        "            for (pos, text, font_scale, font_thickness) in text_positions:\n",
        "                draw_text_with_background(img, text, pos, font_scale, font_thickness)\n",
        "\n",
        "        elif method == 'hierarchical':\n",
        "            img = draw_hierarchical_labels(img, safety_boxes, safety_confidences, safety_class_ids, class_names)\n",
        "\n",
        "        elif method == 'no_text':\n",
        "            # Only draw boxes, no text\n",
        "            pass\n",
        "\n",
        "        elif method == 'side_panel':\n",
        "            img = draw_detections_with_side_panel(img, safety_boxes, safety_confidences, safety_class_ids, class_names, colors)\n",
        "\n",
        "        elif method == 'minimal_text':\n",
        "            # Only show class ID as small text for safety violations\n",
        "            for i, (box, conf, class_id) in enumerate(zip(safety_boxes, safety_confidences, safety_class_ids)):\n",
        "                x, y, w, h = box\n",
        "                cv2.putText(img, str(class_id), (x, y-5),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
        "\n",
        "    return img\n",
        "\n",
        "def process_video_with_clean_labels(video_path, model_path='yolov8n.pt',\n",
        "                                   label_method='smart_positioning',\n",
        "                                   output_path=None, conf_threshold=0.25,\n",
        "                                   show_fps=True, resize_factor=1.0,\n",
        "                                   display_video=True):\n",
        "    \"\"\"\n",
        "    Process video with YOLO detection and clean, non-overlapping labels\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to input video\n",
        "        model_path: Path to YOLO model\n",
        "        label_method: Method for label positioning\n",
        "        output_path: Path to save output video (optional)\n",
        "        conf_threshold: Confidence threshold for detections\n",
        "        show_fps: Whether to display FPS on video\n",
        "        resize_factor: Factor to resize video (1.0 = original size)\n",
        "        display_video: Whether to display video window (set False for Colab)\n",
        "    \"\"\"\n",
        "    # Check if files exist\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found: {video_path}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Error: Model file not found: {model_path}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Load model\n",
        "        print(\"Loading YOLO model...\")\n",
        "        model = YOLO(model_path)\n",
        "        print(\"Model loaded successfully!\")\n",
        "\n",
        "        # Generate unique colors for all classes\n",
        "        num_classes = len(model.names)\n",
        "        colors = generate_unique_colors(num_classes)\n",
        "        print(f\"Generated {num_classes} unique colors for classes\")\n",
        "\n",
        "        # Open video\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open video {video_path}\")\n",
        "            return\n",
        "\n",
        "        # Get video properties\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Validate video properties\n",
        "        if fps <= 0:\n",
        "            fps = 30  # Default FPS\n",
        "        if width <= 0 or height <= 0:\n",
        "            print(\"Error: Invalid video dimensions\")\n",
        "            return\n",
        "\n",
        "        # Apply resize factor\n",
        "        if resize_factor != 1.0:\n",
        "            width = int(width * resize_factor)\n",
        "            height = int(height * resize_factor)\n",
        "\n",
        "        print(f\"Video info: {width}x{height} @ {fps}fps, {total_frames} frames\")\n",
        "\n",
        "        # Setup video writer if output path is provided\n",
        "        out = None\n",
        "        if output_path:\n",
        "            # Create output directory if it doesn't exist\n",
        "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "            # Adjust width for side panel method\n",
        "            output_width = width + 250 if label_method == 'side_panel' else width\n",
        "\n",
        "            # Try different codecs for better Colab compatibility\n",
        "            codecs = ['mp4v', 'XVID', 'MJPG']\n",
        "            for codec in codecs:\n",
        "                try:\n",
        "                    fourcc = cv2.VideoWriter_fourcc(*codec)\n",
        "                    out = cv2.VideoWriter(output_path, fourcc, fps, (output_width, height))\n",
        "                    if out.isOpened():\n",
        "                        print(f\"Using codec: {codec}\")\n",
        "                        break\n",
        "                    else:\n",
        "                        out.release()\n",
        "                        out = None\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if out is None:\n",
        "                print(\"Warning: Could not create video writer. Video will not be saved.\")\n",
        "\n",
        "        # Initialize variables for FPS calculation\n",
        "        prev_time = time.time()\n",
        "        frame_count = 0\n",
        "\n",
        "        print(\"Processing video... Press 'q' to quit\")\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "            # Resize frame if needed\n",
        "            if resize_factor != 1.0:\n",
        "                frame = cv2.resize(frame, (width, height))\n",
        "\n",
        "            try:\n",
        "                # Run YOLO inference\n",
        "                results = model(frame, conf=conf_threshold, verbose=False)\n",
        "\n",
        "                # Extract detections\n",
        "                boxes = []\n",
        "                confidences = []\n",
        "                class_ids = []\n",
        "\n",
        "                for result in results:\n",
        "                    if result.boxes is not None:\n",
        "                        for box in result.boxes:\n",
        "                            # Extract box coordinates\n",
        "                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                            x, y, w, h = int(x1), int(y1), int(x2-x1), int(y2-y1)\n",
        "\n",
        "                            # Validate coordinates\n",
        "                            if w > 0 and h > 0:\n",
        "                                boxes.append([x, y, w, h])\n",
        "                                confidences.append(float(box.conf[0]))\n",
        "                                class_ids.append(int(box.cls[0]))\n",
        "\n",
        "                # Get class names\n",
        "                class_names = model.names\n",
        "\n",
        "                # Draw detections with clean labels using consistent colors\n",
        "                result_frame = draw_yolo_detections(\n",
        "                    frame, boxes, confidences, class_ids, class_names, colors,\n",
        "                    method=label_method\n",
        "                )\n",
        "\n",
        "                # Add FPS counter\n",
        "                if show_fps:\n",
        "                    current_time = time.time()\n",
        "                    if current_time - prev_time > 0:\n",
        "                        current_fps = 1 / (current_time - prev_time)\n",
        "                        cv2.putText(result_frame, f'FPS: {current_fps:.1f}',\n",
        "                                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "                    prev_time = current_time\n",
        "\n",
        "                # Add frame counter\n",
        "                cv2.putText(result_frame, f'Frame: {frame_count}/{total_frames}',\n",
        "                           (10, result_frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                           0.5, (255, 255, 255), 1)\n",
        "\n",
        "                # Write frame to output video\n",
        "                if out and out.isOpened():\n",
        "                    out.write(result_frame)\n",
        "\n",
        "                # Display frame (only if not in headless mode)\n",
        "                if display_video:\n",
        "                    cv2.imshow('YOLO Video Detection', result_frame)\n",
        "\n",
        "                    # Check for quit\n",
        "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                        break\n",
        "\n",
        "                # Progress indicator\n",
        "                if frame_count % 30 == 0:\n",
        "                    progress = (frame_count / total_frames) * 100\n",
        "                    print(f\"Progress: {progress:.1f}% ({frame_count}/{total_frames})\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing frame {frame_count}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Cleanup\n",
        "        cap.release()\n",
        "        if out:\n",
        "            out.release()\n",
        "        if display_video:\n",
        "            cv2.destroyAllWindows()\n",
        "\n",
        "        print(\"Video processing completed!\")\n",
        "\n",
        "        # Display output path if video was saved\n",
        "        if output_path and os.path.exists(output_path):\n",
        "            print(f\"Output video saved to: {output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during video processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "def process_webcam_with_clean_labels(model_path='yolov8n.pt',\n",
        "                                    label_method='smart_positioning',\n",
        "                                    conf_threshold=0.25, camera_id=0):\n",
        "    \"\"\"\n",
        "    Process webcam feed with YOLO detection and clean labels\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load model\n",
        "        model = YOLO(model_path)\n",
        "\n",
        "        # Generate unique colors for all classes\n",
        "        num_classes = len(model.names)\n",
        "        colors = generate_unique_colors(num_classes)\n",
        "        print(f\"Generated {num_classes} unique colors for classes\")\n",
        "\n",
        "        # Open webcam\n",
        "        cap = cv2.VideoCapture(camera_id)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open camera {camera_id}\")\n",
        "            return\n",
        "\n",
        "        # Set camera properties (optional)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "\n",
        "        prev_time = time.time()\n",
        "\n",
        "        print(\"Processing webcam... Press 'q' to quit\")\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Run YOLO inference\n",
        "            results = model(frame, conf=conf_threshold, verbose=False)\n",
        "\n",
        "            # Extract detections\n",
        "            boxes = []\n",
        "            confidences = []\n",
        "            class_ids = []\n",
        "\n",
        "            for result in results:\n",
        "                if result.boxes is not None:\n",
        "                    for box in result.boxes:\n",
        "                        # Extract box coordinates\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                        x, y, w, h = int(x1), int(y1), int(x2-x1), int(y2-y1)\n",
        "\n",
        "                        if w > 0 and h > 0:\n",
        "                            boxes.append([x, y, w, h])\n",
        "                            confidences.append(float(box.conf[0]))\n",
        "                            class_ids.append(int(box.cls[0]))\n",
        "\n",
        "            # Get class names\n",
        "            class_names = model.names\n",
        "\n",
        "            # Draw detections with clean labels using consistent colors\n",
        "            result_frame = draw_yolo_detections(\n",
        "                frame, boxes, confidences, class_ids, class_names, colors,\n",
        "                method=label_method\n",
        "            )\n",
        "\n",
        "            # Add FPS counter\n",
        "            current_time = time.time()\n",
        "            if current_time - prev_time > 0:\n",
        "                fps = 1 / (current_time - prev_time)\n",
        "                cv2.putText(result_frame, f'FPS: {fps:.1f}',\n",
        "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "            prev_time = current_time\n",
        "\n",
        "            # Display frame\n",
        "            cv2.imshow('YOLO Webcam Detection', result_frame)\n",
        "\n",
        "            # Check for quit\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        # Cleanup\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during webcam processing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Usage Examples\n",
        "if __name__ == \"__main__\":\n",
        "    # Example 1: Process a video file (Colab-friendly)\n",
        "    process_video_with_clean_labels(\n",
        "        video_path='/content/drive/MyDrive/client_video/Indorama_pvc_suit.mp4',\n",
        "        model_path='/content/best (3) (2).pt',\n",
        "        label_method='smart_positioning',  # Options: 'smart_positioning', 'hierarchical', 'no_text', 'side_panel', 'minimal_text'\n",
        "        output_path='/content/output_video.mp4',\n",
        "        conf_threshold=0.25,\n",
        "        show_fps=True,\n",
        "        resize_factor=1.0,\n",
        "        display_video=False  # Set to False for Colab to avoid display issues\n",
        "    )\n",
        "\n",
        "    # Example 2: Process webcam feed (uncomment if needed)\n",
        "    # process_webcam_with_clean_labels(\n",
        "    #     model_path='/content/best (3).pt',\n",
        "    #     label_method='smart_positioning',\n",
        "    #     conf_threshold=0.25,\n",
        "    #     camera_id=0\n",
        "    # )\n",
        "\n",
        "    # Example 3: Process video with different methods\n",
        "    # methods = ['smart_positioning', 'hierarchical', 'no_text', 'side_panel', 'minimal_text']\n",
        "    # for method in methods:\n",
        "    #     process_video_with_clean_labels(\n",
        "    #         video_path='/content/VID-20250717-WA0001.mp4',\n",
        "    #         model_path='/content/best (3).pt',\n",
        "    #         label_method=method,\n",
        "    #         output_path=f'/content/output_{method}.mp4',\n",
        "    #         display_video=False\n",
        "    #     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux5iHWf1zX3H",
        "outputId": "4c7a2793-9492-4b88-aed9-2fdfa91f2069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading YOLO model...\n",
            "Model loaded successfully!\n",
            "Generated 9 unique colors for classes\n",
            "Video info: 1920x1080 @ 25fps, 3050 frames\n",
            "Using codec: mp4v\n",
            "Processing video... Press 'q' to quit\n",
            "Progress: 1.0% (30/3050)\n",
            "Progress: 2.0% (60/3050)\n",
            "Progress: 3.0% (90/3050)\n",
            "Progress: 3.9% (120/3050)\n",
            "Progress: 4.9% (150/3050)\n",
            "Progress: 5.9% (180/3050)\n",
            "Progress: 6.9% (210/3050)\n",
            "Progress: 7.9% (240/3050)\n",
            "Progress: 8.9% (270/3050)\n",
            "Progress: 9.8% (300/3050)\n",
            "Progress: 10.8% (330/3050)\n",
            "Progress: 11.8% (360/3050)\n",
            "Progress: 12.8% (390/3050)\n",
            "Progress: 13.8% (420/3050)\n",
            "Progress: 14.8% (450/3050)\n",
            "Progress: 15.7% (480/3050)\n",
            "Progress: 16.7% (510/3050)\n",
            "Progress: 17.7% (540/3050)\n",
            "Progress: 18.7% (570/3050)\n",
            "Progress: 19.7% (600/3050)\n",
            "Progress: 20.7% (630/3050)\n",
            "Progress: 21.6% (660/3050)\n",
            "Progress: 22.6% (690/3050)\n",
            "Progress: 23.6% (720/3050)\n",
            "Progress: 24.6% (750/3050)\n",
            "Progress: 25.6% (780/3050)\n",
            "Progress: 26.6% (810/3050)\n",
            "Progress: 27.5% (840/3050)\n",
            "Progress: 28.5% (870/3050)\n",
            "Progress: 29.5% (900/3050)\n",
            "Progress: 30.5% (930/3050)\n",
            "Progress: 31.5% (960/3050)\n",
            "Progress: 32.5% (990/3050)\n",
            "Progress: 33.4% (1020/3050)\n",
            "Progress: 34.4% (1050/3050)\n",
            "Progress: 35.4% (1080/3050)\n",
            "Progress: 36.4% (1110/3050)\n",
            "Progress: 37.4% (1140/3050)\n",
            "Progress: 38.4% (1170/3050)\n",
            "Progress: 39.3% (1200/3050)\n",
            "Progress: 40.3% (1230/3050)\n",
            "Progress: 41.3% (1260/3050)\n",
            "Progress: 42.3% (1290/3050)\n",
            "Progress: 43.3% (1320/3050)\n",
            "Progress: 44.3% (1350/3050)\n",
            "Progress: 45.2% (1380/3050)\n",
            "Progress: 46.2% (1410/3050)\n",
            "Progress: 47.2% (1440/3050)\n",
            "Progress: 48.2% (1470/3050)\n",
            "Progress: 49.2% (1500/3050)\n",
            "Progress: 50.2% (1530/3050)\n",
            "Progress: 51.1% (1560/3050)\n",
            "Progress: 52.1% (1590/3050)\n",
            "Progress: 53.1% (1620/3050)\n",
            "Progress: 54.1% (1650/3050)\n",
            "Progress: 55.1% (1680/3050)\n",
            "Progress: 56.1% (1710/3050)\n",
            "Progress: 57.0% (1740/3050)\n",
            "Progress: 58.0% (1770/3050)\n",
            "Progress: 59.0% (1800/3050)\n",
            "Progress: 60.0% (1830/3050)\n",
            "Progress: 61.0% (1860/3050)\n",
            "Progress: 62.0% (1890/3050)\n",
            "Progress: 63.0% (1920/3050)\n",
            "Progress: 63.9% (1950/3050)\n",
            "Progress: 64.9% (1980/3050)\n",
            "Progress: 65.9% (2010/3050)\n",
            "Progress: 66.9% (2040/3050)\n",
            "Progress: 67.9% (2070/3050)\n",
            "Progress: 68.9% (2100/3050)\n",
            "Progress: 69.8% (2130/3050)\n",
            "Progress: 70.8% (2160/3050)\n",
            "Progress: 71.8% (2190/3050)\n",
            "Progress: 72.8% (2220/3050)\n",
            "Progress: 73.8% (2250/3050)\n",
            "Progress: 74.8% (2280/3050)\n",
            "Progress: 75.7% (2310/3050)\n",
            "Progress: 76.7% (2340/3050)\n",
            "Progress: 77.7% (2370/3050)\n",
            "Progress: 78.7% (2400/3050)\n",
            "Progress: 79.7% (2430/3050)\n",
            "Progress: 80.7% (2460/3050)\n",
            "Progress: 81.6% (2490/3050)\n",
            "Progress: 82.6% (2520/3050)\n",
            "Progress: 83.6% (2550/3050)\n",
            "Progress: 84.6% (2580/3050)\n",
            "Progress: 85.6% (2610/3050)\n",
            "Progress: 86.6% (2640/3050)\n",
            "Progress: 87.5% (2670/3050)\n",
            "Progress: 88.5% (2700/3050)\n",
            "Progress: 89.5% (2730/3050)\n",
            "Progress: 90.5% (2760/3050)\n",
            "Progress: 91.5% (2790/3050)\n",
            "Progress: 92.5% (2820/3050)\n",
            "Progress: 93.4% (2850/3050)\n",
            "Progress: 94.4% (2880/3050)\n",
            "Progress: 95.4% (2910/3050)\n",
            "Progress: 96.4% (2940/3050)\n",
            "Progress: 97.4% (2970/3050)\n",
            "Progress: 98.4% (3000/3050)\n",
            "Progress: 99.3% (3030/3050)\n",
            "Video processing completed!\n",
            "Output video saved to: /content/output_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    from google.colab import files\n",
        "    files.download(\"/content/output_video.mp4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "-U26kVaP8Qmz",
        "outputId": "b584c6cb-838a-411d-de4c-81cb343700f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e127967b-3dcf-49b9-83f8-90c519f1cec0\", \"output_video.mp4\", 178423643)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}